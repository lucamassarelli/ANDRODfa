#Copyright (C) 2017  Luca Massarelli
#
#This program is free software: you can redistribute it and/or modify
#it under the terms of the GNU General Public License as published by
#the Free Software Foundation, either version 3 of the License, or
# any later version.

#This program is distributed in the hope that it will be useful,
#but WITHOUT ANY WARRANTY; without even the implied warranty of
#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#GNU General Public License for more details.

#You should have received a copy of the GNU General Public License
#along with this program.  If not, see <http://www.gnu.org/licenses/>.

from Classifier import Classifier
from Logger import Logger;
import numpy as np
import os;
import pickle
import random
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.feature_selection import mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectPercentile


##This class implements all method that handle the construction of a malware classification
# model and the evaluation of its performances.

class MalwareClassifier:

    logger = "";
    classifier = ""
    
	#The constructor
    def __init__(self):
        self.logger = Logger(3);
        self.classifier = Classifier();
        
	##This method train the selected classifier.
	# It return the trained model.
	# @param trainData = data for the training
	# @param trainTarget = label for the training data
	# @param classifier = classifier to use
    def trainClassifier(self,trainData,trainTarget,classifier):
        svc = ""
        if(classifier == 'linear'):
            svc = self.classifier.svmLinear(trainData,trainTarget)
        if(classifier == 'sgdSvm'):
            svc = self.classifier.sgdSvm(trainData,trainTarget);
        if(classifier == 'rbf'):
           svc = self.classifier.svmRBF(trainData,trainTarget) 
        if(classifier == 'poly'):
           svc = self.classifier.svmPoly(trainData,trainTarget)
        if(classifier == 'forest'):
            svc = self.classifier.three(trainData,trainTarget)
        return(svc);    
        
	##This method performs the feature selection and then the training of the classifier.
	# It choose the feature subset based on the best accuracy on training stage. Finally, 
	# it apply the classification model to the test data.
	# It return four object:
	# - pred = list of the prediction of the model
	# - bestSvc = model used for prediction
	# - selector = selector of the features
	# - pca = pca object for the dimensionality reduction of the features
	# @param trainData = data for training the classifier
	# @param trainTarget = label for the training
	# @param testData = data for testing
	# @param classifier = type of the classifier to use
    def selectTrainEvaluate(self,trainData,trainTarget,testData,classifier):
        feature_grid = [10,20,30,40,50,60,70]
        scoreMax = 0;
        bestSelector = ' ';
        bestPca = ' ';
        bestSvc  = ' ';
        bestPerc = 0;
        for item in feature_grid:
            self.logger.log("INFO","TRYING FEATURE PERCENTILE " + str(item))
            selector = SelectPercentile(mutual_info_classif, percentile=item);
            trainDataSel = selector.fit_transform(trainData,trainTarget);
            pca = PCA(n_components=0.99, svd_solver='full');
            trainDataSel = pca.fit_transform(trainDataSel);
            result = self.trainClassifier(trainDataSel,trainTarget,classifier)
            self.logger.log("INFO","DONE FEATURE PERCENTILE " + str(item) + " SCORE: " + str(result["score"]))
            if(result["score"] > scoreMax):
                bestPerc = item;
                scoreMax = result["score"]
                bestSelector = selector;
                bestPca = pca;
                bestSvc = result["svc"]
        
        self.logger.log("INFO","DONE FEATURE CROSS VALIDATION");
        self.logger.log("INFO","BEST RESULT WITH " + str(bestPerc) + " PERCENTILE");
        testDataSel = bestSelector.transform(testData);
        testData = bestPca.transform(testDataSel);
        pred = bestSvc.predict(testData);
        return(pred,bestSvc,bestSelector,bestPca);
            
            
    ##This method compute the statistic of the model over a set of predicted label
	# It return an object with 7 keys:
	#	- accuracyMean: accuracy value
	#   - accuracyStd: fixed to 0
	# 	- recall: recall
	# 	- precision: precision
	# 	- f1: f1 score
	# 	- falsePositiveRate: false positive rate 
	#	- N: num of classes
	# @param trueLabel = real label of the test set
	# @param predictedLabel = predicted label over the test set
    def calculateStatistic(self,trueLabel,predictedLabel):
        accuracy = accuracy_score(predictedLabel,trueLabel,normalize=True);
        recall = recall_score(predictedLabel, trueLabel,average = None);
        precision = precision_score(predictedLabel,trueLabel,average=None);
        f1 = f1_score(predictedLabel,trueLabel,average=None);
        falsePositiveRate = self.falsePositiveRate_score(predictedLabel,trueLabel);
        return({"accuracyMean":accuracy,"accuracyStd": 0,"recall":recall,"precision":precision, \
                "f1":f1,"falsePositiveRate":falsePositiveRate,"N":np.unique(trueLabel).shape[0]})

    ##This methods compute the false positive rate
	#it returns a floating number
	# @param x = real label
	# @param y = predicted label    
    def falsePositiveRate_score(self,x,y):
        cnf_matrix = confusion_matrix(x, y);
        fp_rate = np.empty([0]);
        for i in range(0,cnf_matrix.shape[0]):
             it = np.nditer(cnf_matrix, flags=['multi_index'], op_flags=['writeonly'])
             FP = 0;
             TN = 0;
             FN = 0;
             for x in it:
                 if(it.multi_index[0] == i and it.multi_index[1] != i):
                     FN += x;
                 elif(it.multi_index[0] != i and it.multi_index[1] == i):
                     FP += x;
                 elif(it.multi_index[0] != i and it.multi_index[1] != i):
                     TN += x;
             fp_rate = np.append(fp_rate,FP / (FP+TN))
        return(fp_rate);
        
	## This method remove from data all features vectors of package that has minus than 
	# threshold samples in the dataset
	# @param data = matrix of data to clean

	# @param threshold = threshold for deleting data
    def cleanClass(self,datas,threshold):
        data = datas["data"]
        family = datas["family"]
        package = datas["package"]
        familyStr = datas["familyStr"]
        packageStr = datas["packageStr"]
        numOfLabel = np.max(family);
        for i in range(0,int(numOfLabel)+1):
            a = np.where(family == i);
            p = np.unique(package[a]);
            if(p.shape[0] < threshold):
                data = np.delete(data,a[0],axis=0)
                family = np.delete(family,a[0],axis=0)
                package = np.delete(package,a[0],axis=0)
                familyStr = np.delete(familyStr,a[0],axis=0)
                packageStr = np.delete(packageStr,a[0],axis=0)
        return({'data':data,'package':package,'family':family,\
                'familyEncoder':datas["familyEncoder"],'packetEncoder':datas["packetEncoder"],\
                'familyStr':familyStr,'packetStr':packageStr})
        

	## This method select only data that own to families in familyList param
	# @param data = data obj
	# @param familyList = list of family to select
    def selDataByFamily(self,datas,familyList):
        new_data = {};
        dati = np.zeros([0,np.shape(datas["data"])[1]])
        family = np.empty([0]);
        package = np.empty([0]);
        packetEncoder = datas["packetEncoder"];
        familyEncoder = datas["familyEncoder"];
        packetStr = [];
        familyStr = [];
        for item in familyList:
            i = familyEncoder.transform([item]);
            sel = np.where(datas["family"] == i);
            for k in sel[0]:
                dati = np.vstack([dati,datas["data"][k]])
                family = np.append(family,datas["family"][k])
                package = np.append(package,datas["package"][k])
                packetStr.append(datas["packageStr"][k])
                familyStr.append(datas["familyStr"][k])
        return({'data':dati,'package':package,'family':family,\
                'familyEncoder':familyEncoder,'packetEncoder':packetEncoder,\
                'familyStr':familyStr,'packageStr':packetStr})
    
	## This method split the data in training and test set
	# according to the package.
	# @param data = matrix data to split
	# @param label = list of label of data (family)
	# @param package = list of package of data
	# @param testSize = percentile of the test size
    def train_test_split(self,data,label,package,testSize):
        families = np.unique(label);
        trainData = np.empty([0,np.shape(data)[1]])
        testData = np.empty([0,np.shape(data)[1]])
        trainTarget = np.empty([0])
        testTarget = np.empty([0])
        for family in families:
            fam_idx = np.where(label == family);
            package_uniq = np.unique(package[fam_idx]);
            num_packet = np.shape(package_uniq)[0];
            test_pkt = round(num_packet * testSize);
            train_pkt = num_packet - test_pkt;
            for i in range(0,train_pkt):
                pack_chosen = random.choice(package_uniq);
                package_uniq = np.delete(package_uniq,np.where(package_uniq == pack_chosen))
                data_idx = np.where(package == pack_chosen)[0]
                trainData = np.vstack((trainData,data[data_idx,:]))
                trainTarget = np.hstack((trainTarget,label[data_idx]))
            for i in range(0,test_pkt):
                pack_chosen = random.choice(package_uniq);
                package_uniq = np.delete(package_uniq,np.where(package_uniq == pack_chosen))
                data_idx = np.where(package == pack_chosen)[0]
                testData = np.vstack((testData,data[data_idx,:]))
                testTarget = np.hstack((testTarget,label[data_idx]))
        return(trainData,testData,trainTarget,testTarget)
        
                
    
    ##This method print the list of all the data in the datas object structure
	# @param datas = data object structure            
    def listData(self,datas):
        packet,index = np.unique(datas["packetStr"],True)
        count = 1;
        families = []
        for p,i in zip(packet,index):
            f = datas["familyStr"][i]
            print("{} -- Packet: {}, Family: {}".format(count,p,f))
            families.append(f)
            count += 1;
        family,index,indices,occurence = np.unique(families,True,True,True)
        count = 1;
        for f,o in zip(family,occurence):
            f = f.replace('\\r\\n\'','')
            print("{} -- Family: {}, Occurrence: {}".format(count,f,o))
            count += 1
             
	##This methods handles the execution of the classification experiment
	# It return an "experiment dictionary" with several keys:
	# - accuracyMean: average value of the accuracy
	# - accuracyStd: standard deviation of the accuracy
    # - accuracy: accuracy value for each experiment
	# - recall: recall value for each experiment
	# - precision: precision value for each experiment
	# - f1: f1 score value for each experiment
	# - falsePositiveRate: false positive rate value for each experiment
	# - y_pred: last prediction
    # - y_test: real label of the test set
	# - cnf_matrix: average confusion matrix
	# - N: num of classes
	# - svc: classifier model
    # - classes: name of classes
   	# @param dati = data object structure
	# @param numRep = num of independent repetition of the experiment
	# @param classifierName = name of the classifier to use
    def doExperiment(self,dati,numRep,classifierName):
        
        samples = dati["data"];
        family = dati["family"];
        package = dati["package"];
        
        numClass = np.unique(family).shape[0]
        accuracy = np.empty([0])
        recall = np.empty([numClass,0]);
        precision = np.empty([numClass,0]);
        f1 = np.empty([numClass,0]);
        falsePositiveRate = np.empty([numClass,0]);
                                             
        for i in range(0,numRep):
            self.logger.log("INFO","Repetition number: " + str(i))
            
            trainData,testData,trainTarget,testTarget= self.train_test_split(samples,family,package,0.2)
            pred,svc,bestSelector,bestPca = self.selectTrainEvaluate(trainData,trainTarget,testData,classifierName);
            
            accuracy = np.hstack((accuracy,accuracy_score(pred,testTarget,normalize=True)));        
            recall = np.hstack((recall,recall_score(pred, testTarget,average = None).reshape(numClass,1)));
            precision = np.hstack((precision,precision_score(pred,testTarget,average=None).reshape(numClass,1)));
            f1 = np.hstack((f1,f1_score(pred,testTarget,average=None).\
                            reshape(numClass,1)));
            falsePositiveRate = np.hstack((falsePositiveRate,self.\
                                           falsePositiveRate_score(pred,testTarget).\
                                           reshape(numClass,1)));
            if("cnf_matrix" not in locals()):
                cnf_matrix = confusion_matrix(testTarget,pred)
            else:
                cnf_matrix = cnf_matrix + confusion_matrix(testTarget,pred)
            self.logger.log("INFO","Accuracy: " + str(accuracy_score(pred,testTarget,normalize=True)))
            
        u = np.unique(testTarget);
        l = []
        for item in u:
            l.append(dati["familyEncoder"].inverse_transform(int(item)));
            
        return({"accuracyMean":np.mean(accuracy),"accuracyStd":np.std(accuracy),\
                "accuracy":accuracy,"recall":recall,"precision":precision, \
                "f1":f1,"falsePositiveRate":falsePositiveRate,"y_pred":pred,\
                "y_test":testTarget,"cnf_matrix":cnf_matrix,"N":numClass,"svc":svc,\
                "classes":l})
               
	##This method prepare the execution of the experiment, launch it and save result
	# in a file.
	# @param datas = data object to use
	# @param sampleThreshold = threshold to choose family
	# @param numOfRepetition = number of repetition of the experiment
	# @param classifierName = classifier to use
	# @param experimentName = name of the experiment. result file will be name 
	#[experimentName]_i.pkl where is an incrementing index
    def prepareAndRunExperiment(self,datas,numOfRepetition,classifierName, experimentName):
        accuracyMean = np.empty([0])
        accuracyStd = np.empty([0])
        res = datas
        label = res["family"]
        package = res["package"]
        data = res["data"]
        
        self.logger.log("INFO","starting classification experiment: " + experimentName);
        self.logger.log("INFO","repetition: " + str(numOfRepetition));
        self.logger.log("INFO","num of data vector: " + str(data.shape));
        self.logger.log("INFO","num of package: " + str(np.unique(res["package"]).shape));
        self.logger.log("INFO","num of family: " + str(np.unique(res["family"]).shape));
        
        result = self.doExperiment(res,numOfRepetition,classifierName);
        
        result["numDataVector"]=data.shape;
        result["numPackage"]=np.unique(res["package"]).shape;
        result["numFamily"]=np.unique(res["family"]).shape;
        cnt = 0;
        filename = experimentName+"_"+str(cnt)+".pkl";
        while(os.path.exists(filename)):
             cnt += 1;
             filename = experimentName+"_"+str(cnt)+".pkl";

        with open(filename, 'wb') as output:
             pickle.dump(result, output, pickle.HIGHEST_PROTOCOL)
        self.logger.log("INFO","Acuracy mean: {:10.2f} STD: {:10.2f} "\
                        .format(result['accuracyMean'],result['accuracyStd']));
        self.logger.log("INFO","Data saved on file: " + filename);
        
        accuracyMean = np.append(accuracyMean,result['accuracyMean']);
        accuracyStd = np.append(accuracyStd,result['accuracyStd']);
        return({"accuracyMean":accuracyMean,"accuracyStd":accuracyStd});